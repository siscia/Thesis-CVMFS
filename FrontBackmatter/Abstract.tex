%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\pdfbookmark[1]{Abstract}{Abstract}
% \addcontentsline{toc}{chapter}{\tocEntry{Abstract}}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract} 

The problem of software distribution has always been an issue at CERN, given
the massive dimension of the software used for analyzing the data collected
from the LHC. The problem of software distribution has been historically solved
by the use of a distributed, read-only file system like CernVM-FS that allows
distributing binaries to all the geographically distributed data-centers used
for High Energy Physics computation. However, the problem of managing run-time
dependencies is still open, indeed, some application can run perfectly well in
an environment while not working in a different one.

The problem of run-time dependencies is already been solved in the industry
with the use of containers, immutable computing environments which encapsulate
all the run-time dependencies of an application allowing it to run with ease on
different machines. Moreover, containers are becoming more widespread also
inside CERN, suggesting that they will be a key component for running future
High Energy Physics workload. However the integration between containers and
CernVM-FS is not yet mature enough and no automatic tool to manage the whole
lifetime of a container images inside CernVM-FS exists yet.

Indeed, containers are difficult to distribute in an efficient way since their
content is stored inside a few large files. Moreover it has been shown that
most of the content of containers is not used while running the application
itself. Efficient content distribution and managing runtime dependencies should
not be contrasting goals. Hence, in this work, we present a way to efficiently
distribute the content of containers in order to avoid waste of bandwidth and
time.

\newpage

\begin{otherlanguage}{italian}
\chapter*{Sommario}

Il CERN è una organizzazione Europea che opera il più grande laboratorio
        della fisica particellare al mondo.
Il CERN ospita la strumentazione delle 4 più grandi collaborazioni nel campo
        della fisica: ALICE, ATLAS, CMS e LHCb.

Al CERN sono ospitati anche un insieme di collaborazioni nel campo della fisica
        più piccole che beneficiano delle strumentazioni, conoscienze, effetto
        network e servizi disponibili.

Tra i servizi che il CERN offre ai suoi utenti quello informatico è uno dei
        più interesanti. Infatti il CERN ospita e gestisce il WLCG
        \cite{grid:website}, uno dei più grandi centri di calcolo usati per la
        ricerca pubblica.

Un problema che influenza le operazioni dentro il centro di calcolo è
        come installare e impostare il software sui singoli server. Il
        software usato nell ambito scientifico è di grandi dimensioni ed è molto
        dispendioso in tempo di banda e tempo spostare ogni binario da una
        posizione centrale ad ogni singolo computer. Diverse soluzioni sono
        state proposte e alla fine is problema è stato risolto con l'uso di
        CernVM-FileSystem \cite{cvmfs}, un file system di sola lettura che mette a
        disposizione un sistema di distribuzione software che è scalabile,
        solido e che richiede poca manutenzione.

Provedere solo alla distribuzione del software, pero' non è sufficiente.
        Infatti anche le dipendenze a run time devono essere gestite per
        garantire operazioni senza problemi. Una possibile soluzion per gestire le
        dipendenze a run time è l'uso dei containers. I containers
        impacchettano tutte le dipendenze in un file system immutabile così da
        poter eseguire l'applicazione sempre nello stesso ambiente anche se su
        macchine diverse.

Distribuire il contenuto dei container usando CVMFS non è pratico, visto che
        i containers sono distribuiti come un insieme di grandi files e questo
        è contrario ai principi di funzionamento di CVMFS che funziona meglio
        con tanti piccoli file.

Questa tesi esplorerà il problema di create un file system di sola lettura
        addatto per distribuire il contenuto di container sui nodi di
        computazione. Descriveremo la struttura di un file system di sola
        lettura generico e implementero la metodologia proposta con CVMFS.

La tesi è strutturata come segue: 

\begin{itemize} 
        
        \item Il Capitolo \ref{ch:background} fornisce tutte le informazioni
                necessarie di base. Inizia esplorando l'architettura del centro
                di calcolo del CERN, poi spiega il ruolo di CVMFS, la sua
                architettura di alto livello e come e perchè è uno strumentto
                adatto all ambiente del CERN.  Il capitolo illustrerà anche le
                run time di containers che verranno usate in questo lavoro.
                Alla fine forniremo la definizione del problema che questa tesi
                si pone.  
        
        \item Il Capitolo \ref{ch:SoA} descriverà lo stato dell'arte.
                Inizieremo illustrando alcuni file system distribuiti e la loro
                architettura, Poi descriveremo come centri di calcolo di tutte
                le dimensioni sono operati e gestiti. L'ultima parte di questo
                capitolo esplorerà altri progetti nella letteratura che
                lavorano in modo pigro come CVMFS.

        \item Il Capitolo \ref{ch:Methodology} descriverà la metodologia che
                puo' essere usata da un file system di sola lettura generico
                come CVMFS per ospitare immagini di containers e raggiungere
                una distribuzione del contenuto efficiente.

        \item Il Capitolo \ref{ch:Implementation} mostrerà i dettagli della
                implementazione e come il file system è gestito e creato sopra
                CVMFS usando del software apposito.
                
        \item Il Capitolo \ref{ch:Results} mostrerà i risultati di questo
                lavoro comparando il tempo di avvio di containers in scenari
                diversi, così come l'uso di banda.

        \item Il Capitolo \ref{ch:FutureWorks}, infine, esplorerà come questo
                lavoro puo' essere migliorato. Mostreremo anche alcune
                direzioni per lo sviluppo della distribuzione dei containers
                che pensiamo siano di interesse.

\end{itemize}




\end{otherlanguage}

\endgroup

\vfill
