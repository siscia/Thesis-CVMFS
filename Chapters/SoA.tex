%************************************************
\chapter{State of the Art}\label{ch:SoA}
%************************************************

In this chapter we will provide the state of the art for the technologies that are
of interest for this work. In Section \ref{sec:distribute} we will discuss the
state of the art in distributed file system where we will see the major
difference between CVMFS and other distributed file system. In Section
\ref{sec:provisioning} we will explore how provisioning and installation are
done on computing clusters of various size, starting from the smallest up to
the globally distributed one. Finally in Section \ref{sec:soalazy} we will
explore early achievement of working lazily when provision machines.

\section{Distributed File System} \label{sec:distribute}

Several distributed file systems have been proposed and are available. One of
the first implementations of a distributed file system is the Andrew File System
(AFS) \cite{andrew}. It provides a read-write file system, however it guarantees
only weak consistency, hence read and write operations are directed only to
locally cached copy of the file. The changes are propagated to the server when
a file is eventually closed. AFS informs the client if a different user modifies a
cached file using callbacks, but those callbacks are discarded after any
communication error between the client and the server, including network
failures or timeouts. The need to keep a constant connection open between the
client and the server makes the client/server ration quite small, ~200. Hence
for every 200 clients a new server machine is needed.

Another commercial distributed file system is BeeGFS \cite{beegfs} it has a
different approach than AFS, since it is composed by three different
components: the clients, the metadata servers and the storage servers. The
client coordinate and informs the metadata servers which in turns coordinate
the storage servers that, finally, are accessed by the clients.  Also this
architecture of distributed file system relies on metadata components that
bounds the number of clients imposing an overhead.

Another approach to store huge amount of data is Amazon S3 (Simple
Storage Service) \cite{aws-s3}. S3 does not provide a classical POSIX interface
but an eventually consistent REST interface. Objects are created immediately
but deletions will eventually happen in the future. Moreover it has no knowledge
of permissions outside of a simple access control list, symbolic links or even
folders. Due to this shortcomings S3 is not directly usable to distribute and
instal software one HPC clusters, however it can be used as a simple layer to
access the file leaving all the metadata handling to CVMFS. As mentioned in
Section \ref{sec:cvmfs} the files provided by CVMFS are accessed using the HTTP
protocol. And they don't have to be in a web server managed by CVMFS but they can
be in any accessible location.

To the best of our knowledge, no distributed file system handles metadata like
CVMFS does, indeed all the metadata queries to CVMFS are handled by the client
themselves reading the catalogs files, hence the metadata operations are not a
limitation anymore since they scale out with the number of clients. Of course
this is possible since CVMFS provides to the clients only a read interface and
the write are all serialized in the CVMFS server component.

\section{Provising of Machines} \label{sec:provisioning}

In this section we are going to explore how it is possible to provision and
install software on cluster of machines. The proposed methodologies will grow
in complexity as the size of the cluster grows. In parallel we will see how the
problem shifts from the handling of files to the handling of metadata.

In Section \ref{subsec:soasmall} we will describe how installation and
provisioning of machines is done in humble set up with very few machines. The
need of automation will arise on bigger clusters, hence in Section
\ref{subsec:soacluster} we will describe how automatic tools help in
provisioning local clusters. In Section \ref{subsec:soawlcg} we will explore
the solution that were applied to the WLCG before CVMFS and their weaknesses.
Finally in Section \ref{subsec:soaruntime} we will explore proposed solution to
the problem of managing run time dependencies in a globally distributed cluster
like the WLCG.

\subsection{Single o Local Machines and Small Clusters} \label{subsec:soasmall}

Several systems have been proposed to manage installation and provisioning of
machines.

The simpler approach is of course the manual approach were the source code is
downloaded on the local machine, compiled and finally installed. While this
works fine for the expert user, it introduces a lot of difficulties and
friction for users that are not experts. The compilation part is error prone
and it may require dependencies, so the user must be ready to do a lot of
research and even reading makefiles if necessary. Moreover, if the software has
run-time dependencies, it is necessary for the user to install them as well,
and so recursively.

Of course there is no need to re-compile the software every time, moreover
compilation is an expensive and quite technical step that not all users are
confident in doing.  Indeed the package managers \cite{yum}, \cite{apt} solves
this kind of problem, the software is compiled once by expert on the software
itself and distributed along with a script that install it in the most
appropriate location in the system and with a series of dependencies. This is a
major shift in the interface since now the user simply installs a package
without caring of all its dependencies or without even needing a compiler. It
is the package manager that will eventually install run time dependencies,
directly or indirectly.

The distribution of packages poses already a challenge, if enough users are
interested in the packages hosted in a single package manager, it would be
necessary to load balance the request between several mirrors site, indeed this
is what happens in big linux distribution that serves a lot of users. The
package manager itself is mirrored to different data center each one serving a
part of the traffic.

\subsection{Local cluster} \label{subsec:soacluster}

As long as the installation of software is manual and done in relatively small
clusters, package managers are suitable for hosting all the software and
provision all the servers.

When the cluster starts to grow, manual installation is not a solution anymore,
indeed specialized tools are available to manage the state of a fleet of
machines.

\textit{Ansible} \cite{ansible}, \cite{provisioning} is a tool that helps in
managing the installation of software in a fleet of machines. It is based on two
simple concepts, the \textit{inventory} and the \textit{playbook}. The
inventory associates each machine that is managed to a set of tags. The playbook
is a set of statuses each associated with a tag. When provisioning a fleet of
machines, Ansible tries to adapt the status of each server in a way that it
respects the status declared in the playbook. As an example in the playbook we
may declare that the servers tagged as \texttt{web-server} need to have the
\texttt{apache} service on, while the servers tagged as \texttt{build-node}
needs to have the \texttt{gcc} package installed. In the first case Ansible
will try to start the \texttt{apache} service while in the second case it will
try to install the \texttt{gcc} package.

A possible issue with Ansible is about the use of those servers: users may
login into the server, change their configuration to fix an urgent problem and
then do not update the Ansible playbooks. In the long term this causes a drift
on configuration and settings that makes quite difficult to manage the fleet of
services.

A solution to the drift in configuration is the use of a different tool,
\textit{Puppet} \cite{puppet}, \cite{provisioning} which works on a similar
principle of Ansible but it constantly monitor the status of each machine
reverting any changes.

If different users need to access a shared pool of computing resources, another
solution consist in the use of \textit{virtual machines}. Each physical machine
gets partitioned into a set of different, isolated virtual machines where each
users can install and configure its software. Moreover the image of the virtual
machine itself may come with software already installed and configured.

Providing software already installed in a machine is a major conceptual shift
where we move from the user having the responsibility to install all the needed
software to a model where the software is already in the system itself.
However, virtual machines while providing the stronger level of isolation
possible in a shared environment are slow to set up and also slower than native
performance.

An evolution of virtual machines are the containers: containers provide a
smaller level of isolation than virtual machines but are much quicker to set up
and have close to native performance. Moreover containers are simple to build
and their popularity is raising between developers as well. Indeed it is
possible to set up a container that encapsulates all the status necessary to
run a specific computation: this allows to move the container from a machine to
another without worrying anymore of run-time dependencies.

All these solutions work well in local clusters, however all of them requires
to move all the necessary software on a local machine, also components that are
not needed. Starting a fleet-wide installation using tools like
\textit{Ansible} or \textit{Puppet} means that the package managers needs to
serve at roughly the same time all the packages necessary by all the machines,
which can quickly sum up to a lot of bandwidth. The use of virtual machines or
containers is not better in this cases since it still requires to move a lot of
content. In big cluster it is common to provide local package managers and local
container registries.

Providing the content to several different machines is a problem that can be
solved by a careful use of caches and optimizations. Diving deeper into the
containers world, the OCI standard defines that each layer is identified by a
single unique digest. This helps since it provides an unique identifier to
cache and, more importantly, by bundling together a lot of different files in a
single tar files it sidesteps the need of managing metadata for all the files in
a container. Still this comes at the cost of needing to download a big file
even if its content is not all necessary. Similar consideration can be done for
the provisioning of Virtual Machines.

\subsection{WLCG environment} \label{subsec:soawlcg}

With the respect of provisioning the WLCG and its modus operandi is an harsher
environment that local cluster. Indeed it operates in a batch computation mode,
where jobs are scheduled and started all together in a wide fleets of machines,
possibly in geographical different data centers. A provisioning structure that
relies on package managers or simple containers will witness a huge peak in
the bandwidth request that will slow down the provisioning itself.  Moreover
scientific software is usually quite large, also in the order of 20GB and the
provisioning of relatively few machines is sufficient to cause a big spike in
the bandwidth request. Indeed several solution have been proposed and tested to
address this issue.

The \textit{Andrew File System} \cite{andrew} (AFS) is a distributed read-write
file system optimized for home directories on globally distributed
workstations. AFS provides a global name space (/afs) partitioned into
\textit{cells} and \textit{volumes}. Scientific collaborations can host the
authoritative copies of their experiment software on public volumes (cell
/afs/cern.ch). Using AFS, scientific software need to be installed only once in
the server in order to be used by several clients.  Since AFS needs to keep the
connection state for all its clients it has a low client/server ration with is
limited to about 200:1.

Another proposed mechanism to manage installation on the WLCG are the
\textit{grid installation jobs}, described in \cite{jakob:cvmfs}. Jobs that
pre-install software releases on worker node. In order to avoid to install the
software on every single node WLCG Tiers provides a \textit{share software
areas}, a NFS volume mounted to every worker node that can be used to provide
the software itself. While the \textit{share software areas} are a necessity,
they also introduce a single point of failure and high chance to overload the
NFS with meta-data operations.

An evolution of the \textit{grid installation jobs} is the \textit{ALICE
Software Installation System} described in \cite{jakob:cvmfs}. The software
releases are still distributed as packages but those packages are physically
distributed to the worker nodes using the BitTorrent protocol. This improves
the installation times using all the bandwidth not only from the package
manager through the worker nodes but also between the worker node themselves.
Unfortunately this architecture required to still transfer all the packages to
all the nodes which is a waste of bandwidth, introduce sources of errors and it
is time consuming.

Another evolution of the \textit{grid installation jobs} is \textit{GROW-FS}
\cite{grow-fs} which pioneered the idea of meta-data handling on the worker
nodes and not in a central node. \textit{GROW-FS} provides as well a shared
areas for installation using a simple web server. \textit{GROW-FS} uses a
single catalog for the entire directory tree and changes to the file system
structure requires to completely reconstruct the catalog as well as re-mounting
the file system on the worker nodes.

\subsection{Managing run time dependencies on the WLCG} \label{subsec:soaruntime}

Other than the distribution of content, CERN faced the problem of managing
run-time dependencies. A possible solution to this problem would be to
statically link all the dependencies, but this would generate extremely big
binaries.  Another solution would be to carefully managing all the software
installed on the machines, including all the recursive dependencies. However this
clash with the WLCG model where jobs can migrate from a data center to another
along with their dependencies.  Moreover sometimes even a very careful
installation is not sufficient since it is possible that two application that
could run on the same machine have clashing dependencies.

The problem of how to manage runtime dependency is been solved outside CERN in
several different ways.

On small scale it is possible to use simple package managers that automatically
install the dependencies, however they suffer of few different problem. First
and foremost in case of dependencies clashing the package manager simply refuse
to install the required package, which is not an acceptable solution. Then at
the scale of the WLCG the package manager quickly become a bottleneck with
respect to the bandwidth necessary to distribute the content but also with
respect to the size of the internal databases.

Another possible solution is the use of containers. Packaging all the runtime
dependencies along the application code in an immutable file system allow
containers to exactly reproduce the same environment and condition to ensure
smooth operations. The use of containers however is not convenient in very
large clusters like the WLCG, indeed the content necessary to run a container
is distributed as few large tar files and this makes the distribution itself
inefficient since a lot of content not useful for the application itself is
downloaded anyway. 

Several system have been propose to decrease the downloading time of containers
images, however all of them work \textit{ahead of runtime} trying to optimize
how fast the overall system is capable of delivering the whole image into
the host. FID (Faster Image Distribution) \cite{FID} is a P2P Docker images
distribution system that is able to accelerate the speed of distributing Docker
images by taking full advantage of the bandwidth of not only the Docker
Registry but also of the other nodes in the cluster, while decreasing the
downloading time. However,  FID still require to download all the image before to run the
computation. Another work by Anwar et all. \cite{210500} characterize the
workload of large-scale registries in order to derive design implications for
more optimized registries, still they are working to optimize the time necessary
to serve and download the whole image from the registry.

While the aim of this work is still to decrease the start up time of uncached
containers we took a different road. The previous works focus on optimizing
the delivery time of the whole content image, we decided to focus on minimizing
the amount of content that the client needs to download which of course leads to
shorter start up time for the containers and also to save bandwidth.

\section{Working lazily} \label{sec:soalazy}

The experience with the installation mechanisms tested at CERN suggests that it
is necessary to work lazily in order to provide software and content to a wide
fleet of machines and this is exactly what CVMFS does for us. However we need
another step in order to solve also the problem of run time dependencies.

Few other system have been proposed to address the problem using a similar
architecture. The \textit{cvmfs/graphdriver} \cite{graphdriver-plugin} allows
us to run Docker images starting from a \textit{recipe} file that  list the
layers to mount and their location in the file system. Unfortunately a way to
provide the \textit{thin images} was not provided, moreover it was not provided
a way to structure the file system itself. Slacker \cite{slacker} tackle the
same problem but it uses a very different implementation. Their work is based
on NFS and flatten layers. All the layers of an image get flattened into a
single layer, and such layers stored as a single file into a NFS shared between
the Docker Registries and the Docker Client. While \textit{cvmfs/graphdriver}
shares a set of layers as Docker images, Slacker share a simple reference to
the file stored in the NFS. Slacker is able to reach very interesting
performance thanks to the implementation of the NFS they use that relies on
network disks. We lack the details about the NFS used by Slacker but we believe
that their performance may be strongly influenced by the size of the cluster.

Similarly to Slacker in another work Nicolae et al. \cite{back-forth} proposed
another lazy structure this time to run Virtual Machines. They trap the IO to
the Virtual Machine image using FUSE and redirect those call into a shared
read-write file system backed by local disk on each machine.  Similarly to
Slacker we believe that this approach works well on a small set of machines but
it won't scale well on a big cluster. Indeed their experimental set up was
limited to 120 machines. As mentioned above for the \textit{grid installation
jobs} the use of NFS brings the risk of overloading the NFS itself with
metadata operations.

In this work we aim to finally bridge the two worlds of containers and
distributed read-only file system like CVMFS. Instead of working with blocks
like Slacker and Nicolae et al. our primitives will be files. Similarly we are
not going to provide a read-write interface to the worker node since this
will definitely impact the scalability of the solution given the necessities to
implements locks. The client will only be able to read the files.

