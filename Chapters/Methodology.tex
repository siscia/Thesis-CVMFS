%************************************************
\chapter{Methodology}\label{ch:Methodology}
%************************************************

In this chapter we are going to analyze the proposed filesystem structure for using both Singularity images and docker thin-images. 

We will start by analyzing the structure for Singularity and then we will move on to the docker thin-images.

\section{Singularity Images}

As mentioned in \ref{subsec:singularity-docker-distribution} Singularity is able to run standard docker images, moreover, one of the most common way to distribute Singularity images is to make them available as Docker images in Docker registries.

Hence we exploited the natural hierarchical structure of docker images in order to enhance the discoverability of the Singularity images. 

The first level of the hierarchy is the docker registry where the image is hosted. The most common registries in our case are the official docker hub (\texttt{registry.hub.docker.com}) and the CERN internal registry (\texttt{gitlab-registry.cern.ch}). 

The second level in the structure is the namespace of the docker image. 
If the image is one of the official docker images it will be the standard namespace: “library”. 
In all the other cases, the namespace will be the same as the original docker registry. 
For example for the images belonging to the ATLAS collaboration the namespace \texttt{atlas} is used.

The last level is the name of the image itself together with the tag of such image, separated by a colon (\texttt{:}).
We decided to avoid yet another level containing just the tags. 
Indeed there are relatively few tags for each image and adding another level of indirection would have made it harder to explore the filesystem.
Moreover, we opted to use the colon because it is the same character used in the docker registries between the images and the tag and it is immediately recognizable by users.

In the image \ref{fig:simple-fs} we offer a visualization of the filesystem for a small repository. It is possible to appreciate the hierarchical structure. We will continue this part explaining the use of symbolic links.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch.
.2 registry.hub.docker.com.
.3 library.
.4 foo:bar -> /cvmfs/unpacked.cern.ch/.flat/ab/abdkwe...fejfne.
.4 baz:latest -> /cvmfs/unpacked.cern.ch/.flat/02/02vbfd...bvebv.
}
\caption{Visualization of the Filesystem structure, the arrows indicate symbolic links}
\label{fig:simple-fs}
\end{figure}

While this structure is user friendly, it makes the maintenance of the repository complex.

The tags used in each image are not immutable, hence, without continuous maintenance, it may happen that the images stored inside the filesystem are not up to date.
However, with the described structure, it would be extremely complex to detect if an image is up to date or if it needs further updates. 

Fortunately each image is uniquely identified by its digest. 
We decided to store the real content of the images in an hidden folder that embed the digest itself. 
We preserved the structure presented above using symbolic links.

The folder that contains the real content of a Singularity images are all below the standard subdirectory \texttt{.flat/}.
The name \texttt{.flat/} was chosen to make it clear that only flatted file systems are stored in there.

From a theoretical point of view it would be sufficient to store the whole content of the Singularity images in the folder \texttt{.flat/\$image\_digest}. 
However, from a practical point of view this would create too much content in a single folder putting too much pressure in the CVMFS sub-catalog system.

The standard solution in cases like this is to create a fixed number of directories and put some of the content in each of them. 
We decide to use the first 2 bytes of each digest as “super-directory”.

Since the digest is an hexadecimal string this approach provides us with $16 \times 16 = 256$ fixed subdirectories inside the \texttt{.flat/} directory, each of which will contain only the content of the images whose digest start with those 2 specific bytes.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch/.flat.
.2 0c.
.3 0cnjrnjdnjnbirnfddnvk.
.3 0cji2jqwdcrnbjvevjofr.
.2 a4.
.3 a4nvjrbnjvbnrjbnjrtnb.
.3 vnrjnvjnrbjvnjrnbvjrn.
}
\caption{Visualization of the "super directories" in the ".flat" subdirectory}
\label{fig:super-directories}
\end{figure}

On figure \ref{fig:super-directories} we can see that “0c”, “2c”, …, “ea” are all “super-directories” and each one contains only the filesystems that start with “0c”, “2c”, …, “ea” respectively.
Note the case of “ea” that contains filesystems of multiple images whose digest start with “ea”.  

Another positive side-effect of the use of symbolic links is that symbolic links manipulation is defined as atomic in the POSIX standard.

We re-iterate that the use of "super-directories" is necessary for limits in the implementation of CVMFS and they are not necessary on an abstract read-only filesystem.

\section{Docker Thin Images}

While for Singularity is sufficient to have the image unpacked in a simple directory running docker containers requires a more complex set up.
As explained in \ref{subsec:docker-thin-images} the recipe of the docker thin-image contains the path of the directories where each layer of the original docker image is hosted, those directories will be mounted by a specific docker plugins, hence need to be accessible.

In the case of docker we do not need to provide an human-friendly interface for the filesystem like we did for Singularity. Indeed the filesystem won't be explored by humans but it will only be accessed by the docker daemon plugin.

Like docker images also the docker layers are identified by an unique digest.

Since docker layers and Singularity images shares similar characteristics and access patterns their structure in the filesystem is quite similar.

All the docker layers are stored under a common subdirectory of the filesystem, the \texttt{.layers/} directory.

Similarly, in order to don’t put too much pressure in the CVMFS distribution systems, each layer is stored in a “super-directory” following the exact same schema of the Singularity images.

A big advantage of the use of layers over flat images, is that layers can be shared by multiple images.

This sharing provides both an advantage and a disadvantage:
\begin{enumerate}
        \item It is a great opportunity to avoid re-doing work that is already been done. In particular, if a layer is already in the filesystem it will not be added again.
        \item It makes more complex to remove a docker image. To remove an image is necessary to remove each layer, however layers may be shared between images.
\end{enumerate}

Removing layers has the important implication that once the layer is removed every thin image that relies on it won’t work anymore. However those thin-images could be stored on the client side where we don’t have any access.

We consider several option:
\begin{enumerate}
\item Never remove layers
\item Remove layers as soon as possible
\item Provide a grace period before to finally remove the layer
\end{enumerate}

The option to never remove layers is impractical since the size of the filesystem will grow unbounded.

Remove layers as soon as possible was not desiderable, even running computation could be broken by this policy and the users have no way to deal with this possibility, but retrying the whole computation.

The last option seemed the most sensible and better suited for our use case, and so it is the one that we implemented, this gave users the possibility to:
\begin{enumerate}
\item Complete their computation
\item Update the local images in order to always run stable containers
\end{enumerate}

In order to know which layer delete from the filesystem we store a reference that map each layer to the images that use the layer itself.

This reference is stored as metadats in a simple \texttt{.json} file.

Anytime a new image is added to the filesystem at every layer used by the image we add the reference of the image itself.

\begin{lstlisting}[caption={Algorithm to add an image reference to the layer metadata}, label={lst:add-image-reference-to-layer}]
Function AddReferenceToImage
        Pass In: LayerReference, ImageReference
        ReferenceFile := FindReferenceFile(LayerReference)
        if ReferenceFile exist
                References := LoadReferenceFromFile RefereceFile
                Add ImageReference to References
                Overwrite References to ReferenceFile
        else 
                References = ImageReferences
                Write References to ReferenceFile
        endif
EndFunction
\end{lstlisting}

When we decide to remove an image, for any layer we check that it is used only by the image we want to remove, if such is the case, we remove the layer, if it is not the case we just remove the reference that mapped the layer to the image to remove.

\begin{lstlisting}[caption={Algorithm to remove an image from the filesystem}, label={lst:remove-layer}]
Function RemoveLayer
        Pass In: LayerReference, ImageReference
        ReferenceFile := FindReferenceFile(LayerReference)
        References := LoadReferenceFromFile RefereceFile
        Remove ImageReference from References
        if size References == 0
                Remove Layer
        else
                Overwrite References to ReferenceFile
        endif
EndFunction
\end{lstlisting}


\section{Storing metadata information about layers}

An additional directory layer is used to store metadata information. 
Below the directory called as the digest of the layer there are two more directories: 
\begin{enumerate} 
        \item \texttt{rootfs/} directory that actually store the content of the layer
        \item \texttt{.metadata/} directory that stores the references to the image in a simple JSON encoded file, “origin.json” 
\end{enumerate}

Of course, the recipe of the thin images is not concerned at all with the content of the \texttt{.metadata/} directory. Hence the recipe files points directly to the \texttt{rootfs/} directory.
