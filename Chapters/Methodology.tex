%************************************************
\chapter{Methodology}\label{ch:Methodology}
%************************************************

In this chapter we will introduce a read-only file-system structure for
running Docker images using both Singularity and the docker thin-images plugin.
We focus on Singularity because it is a widely deployed system in HPC and on
Docker thin-images because it allows the user to leverage the Docker infrastructure
to run images whose content is distributed with a read only file-system.

Section \ref{sec:methodology-high-level} will provide a high level overview of
the proposed methodology explaining why we decide to focus our design on CVMFS,
Singularity and Docker with the \textit{cvmfs/graphdriver} plugin.

Section \ref{sec:methodology-singularity} will analyze the file-system
structure to host the Docker images to run with Singularity. We will explain
how we created a hierarchical structure similar to the one of the docker
registries while keeping the repository maintainable and without putting too
much pressure in the sub-catalog system of CVMFS.

Section \ref{sec:methodology-docker} will analyze how we stored the content used
by the thin-images Docker plugin. Structural similarities between Docker images
and Docker layers will drive us to adopt a very similar solution to avoid
stressing the sub-catalog system. The sharing of layers between different
docker images will allow us to avoid repeating work, but it will introduce
difficulties during the deletion of the image itself that we will solve using a
reference count system.

Finally Section \ref{sec:methodology-keep-track} will show how we kept
track of the images already present in the file-system.

\section{High Level overview of the proposed file system}
\label{sec:methodology-high-level}

As mention the aim of this work is to provide an efficient content distribution
system to run containerized application on big distributed cluster.  The
content distribution part will be managed by CVMFS which provides us with the
primitives necessary to efficiently distribute the content, moreover, up to our
knowledge, CVMFS is the only stable and tested system used that allows to
distribute lazily and efficient Terabytes of data.  Then, there are two
container run-times that we can use to actually run the containers.
Singularity, introduced in Section \ref{subsec:singularity} that allow to run
containers whose content is already unpacked in a simpler folder. Docker with
the \textit{cvmfs/graphdriver} plugin which allows Docker to run \textit{thin
images} made of a simple \textit{recipe} \texttt{json} file with describe where
in the file system find the necessary layers that are needed to be mounted
before to start the container itself.

We will provide a structure for a read-only file system that allow to execute
containers using both the container run-times mentioned above. The proposed
structure is absolutely generic and even if not implemented with CVMFS it would
allow to the above container run-times to work with minimal modifications
\footnote{The implementation of \textit{cvmfs/graphdriver} relies on a specific
\textit{cvmfs} url in the \textit{recipe} format which is a simple
implementation details that can be easily generalized.}, however some choice of
the directory structures have been taken given the design of CVMFS and may not
be necessary on a generic read-only file system.

Moreover, in Chapter \ref{ch:Implementation} we will close the last gap in the
\textit{cvmfs/graphdriver} plugins providing a way to generate \textit{thin
images} starting from standard Docker \textit{fat images} and expose them in
the presented file system.

The proposed file-system provides two main structure. A non-hidden set of
directories that host the unpacked images to run with Singularity and a hidden
folder to host the layers used by the \texttt{cvmfs/graphdriver} Docker plugin.
Along with these structure another hidden directory will contains metadata
information about the Docker images already in the repository.

The directories that host the unpacked Docker images have the same hierarchical
structure of the Docker structure as mentioned in \ref{sec:containers}, hence
the first directory is the name of the registry that host the image, it follow
the namespace which refers to the user of the organization responsible for the
image and the last level is the image name along with the tag.

The directory that host the layers of the Docker images can conceptually be a
flat structure simply containing the layers each identified by its digest.
However a simple flat structure will put too much pressure in the catalog
system so we will aggregate layers that share the same digest prefix and create
a sub-catalog for each aggregation.

The hidden metadata folder will follow the same hierarchical structure of
Docker images that allow to quickly locate the metadata information of an image
given just the name of the image itself.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch.
.2 registry.hub.docker.com.
.3 library.
.4 centos:centos7.
.2 .layers/.
.2 .metadata/.
}
\caption{High level visualization of the proposed file-system}
\label{fig:high-level-fs}
\end{figure}


\section{Singularity Images}
\label{sec:methodology-singularity}

To run Docker images using Singularity is sufficient to start the singularity
executable providing as input the directory where the image is been unpacked.
In this section we are going to show how we structure the file-system in a way
that allow users to easily discover and run unpacked docker images using
Singularity while keeping the file-system easy to maintain.

As mentioned in \ref{sec:containers} docker images have
a hierarchical structure.  The first level of the hierarchy is the docker
registry where the image is hosted.  The most common registries in our case are
the official docker hub \footnote{\texttt{registry.hub.docker.com}} and the
CERN internal registry \footnote{\texttt{gitlab-registry.cern.ch}}.

The second level in the hierarchical structure is the namespace of the docker
image.  If the image is one of the official docker images it will be the
standard namespace: “library”.  In all the other cases, the namespace will be
the same as the original docker image.  For example for the images belonging to
the ATLAS collaboration we use the namespace \texttt{atlas}.

The last level is the name of the image itself together with the tag of such
image, separated by a colon (\texttt{:}).  We decided to avoid yet another
level containing just the tags.  Indeed there are relatively few tags for each
image and adding another level of indirection would have made it harder to
explore the file-system.  Moreover, we decided to use the colon because it is
the same character used in the docker registries between the images and the tag
and it is immediately recognizable by users.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch.
.2 registry.hub.docker.com.
.3 library.
.4 centos:centos7 -> /cvmfs/unpacked.cern.ch/.flat/75/75835...0c4ab6d.
.4 centos:latest -> /cvmfs/unpacked.cern.ch/.flat/75/75835...0c4ab6d.
.4 debian:stable -> /cvmfs/unpacked.cern.ch/.flat/a4/a4274...ba594cb.
.4 gcc:latest -> /cvmfs/unpacked.cern.ch/.flat/ce/ceccd...a75dd28.
.4 openjdk:9 -> /cvmfs/unpacked.cern.ch/.flat/5a/5adaf...5344d70.
.4 python:2.7 -> /cvmfs/unpacked.cern.ch/.flat/3c/3c43a...0e7c9d4.
.4 python:3.4 -> /cvmfs/unpacked.cern.ch/.flat/43/43953...ed73435.
.3 efajardo.
.4 docker-cms:tensorflow -> /cvmfs/unpacked.cern.ch/.flat/2d/2d5b4...97d44fc.
}
\caption{Visualization of the Filesystem structure, the arrows indicate symbolic links}
\label{fig:simple-fs}
\end{figure}

While this structure is user friendly, it makes the maintenance of the
repository complex.

The tags used in each image are not immutable, hence, without continuous
maintenance, it may happen that the images stored inside the file-system are
not up to date making difficult for the user to know what version of the
software is being run.  However with the described structure, it would be
extremely complex to detect if an image is up to date or if it needs further
updates.

To work around this issues we exploited the fact that each image is uniquely
identified by its digest.  Indeed we decided to store the real content of the
images in an hidden folder that embed the digest itself while preserving the
structure presented above using symbolic links.

We show the directory structure of the file-system on figure
\ref{fig:simple-fs}.

The folder that contains the real content of a Singularity images are all below
the standard subdirectory \texttt{.flat/}.  The name \texttt{.flat/} was chosen
to make it clear that only flatted file systems are stored in there.

Embedding the digest in the name of the folder allows to immediately find the
location of an image, which is useful when an image become obsolete and need to
be deleted from the file-system.

From a theoretical point of view it would be sufficient to store the whole
content of the Singularity images in the folder \texttt{.flat/\$image\_digest}.
However, from a practical point of view this would create too much content in a
single folder putting too much pressure in the CVMFS sub-catalog system.

To overcome this issue we decided to create a fixed number of
"super-directories" where we placed the unpacked folder of the images.  To
easily locate each unpacked folder in the super-directories we decided to call
each super-directory as the prefix of the digest of the images it is
containing. Since the digest is an hexadecimal string this approach provides us
with $16 \times 16 = 256$ fixed super-directories inside the \texttt{.flat/}
directory, each of which will contain only the content of the images whose
digest start with those 2 specific bytes.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch/.flat.
.2 0c.
.3 0cbf37812bff083eb2325468c10aaf82011527c049d66106c3c74298ed239aaf.
.2 2c.
.3 2cc378c061f7b3e8d9096728eb75722a89f31fb3f3117ed10c66cc2f4b8ab281.
.2 5a.
.3 5adaf00da2a3cf6b611e7c850778fad3dc62c548864706b822b5f3ce65344d70.
.2 ea.
.3 ea4c82dcd15a33e3e9c4c37050def20476856a08e59526fbe533cc4e98387e39.
.3 eadfca9546a132104b8bdb6b76952c6e5d412301704b7bc94e9176bcc5dda0fe.
}
\caption{Visualization of the "super directories" in the ".flat" subdirectory}
\label{fig:super-directories}
\end{figure}

On figure \ref{fig:super-directories} we can see that “0c”, “2c”, …, “ea” are
all “super-directories” and each one contains only the file-systems that start
with “0c”, “2c”, …, “ea” respectively.  Note the case of “ea” that contains
file-systems of multiple images whose digest start with “ea”.

However, to relieve pressure from the catalog system is not sufficient to
simply aggregate the images into "super-directories", we also need to create a
sub-catalog for each "super-directory." Moreover, since each image can contains
itself a lot of files we decide to create a sub-catalog also for each unpacked
image.

Another positive side-effect of the use of symbolic links is that symbolic
links manipulation is defined as atomic in the POSIX standard.

The use of "super-directories" is necessary for limits in the implementation of
CVMFS and they are not necessary on an abstract read-only file-system.

\section{Docker Thin Images}
\label{sec:methodology-docker}

While for running docker images using Singularity it is sufficient to have the
image unpacked in a simple directory running docker containers using the
thin-images plugin requires a more complex set up.  As explained in
\ref{subsec:docker-thin-images} the recipe of the docker thin-image contains
the path of the directories where each layer of the original docker image is
hosted, those directories will be mounted by the docker plugin.

All the docker layers are stored under a common subdirectory of the
file-system, the \texttt{.layers/} directory.

Since the sub-tree of the file-system used by the Docker thin-images is used only
by the Docker plugin we don't need to create a human-friendly structure like we
did for the Singularity sub-tree.

Like docker images also the docker layers are identified by an unique digest,
and similarly to the docker images, store all of them in a single directory
will put too much pressure in the CVMFS sub-catalog system, hence we follow the
exact same model used for storing the unpacked images also for the layers
creating 216 super-directories.

A big advantage of the use of layers over flat images, is that layers can be
shared by multiple images.

The sharing of layers allow us to avoid re-doing work that is already been
done, in particular if a later is already in the file-system it will not be
added again. On the other hand it makes more complex removing an image since it
is necessary to remove each layer that compose the image, but some layer may be
shared between images.

Removing layers has the important implication that once the layer is removed
every thin image that relies on it won’t work anymore.  However those
thin-images could be stored on the client side where we don’t have any access.
Please refer to the figure \ref{fig:thin-image-lifecycle} on page
\pageref{fig:thin-image-lifecycle}

To do not disrupt the user workflow while keeping the repository to a manageable
size we consider several option: 
\begin{enumerate}
\item Never remove layers
\item Remove layers as soon as possible
\item Provide a grace period before finally removing the layer
\end{enumerate}

The option to never remove layers is impractical since the size of the
file-system will grow unbounded.

Remove layers as soon as possible is not desiderable, even running computation
could be broken by this policy and the users have no way to deal with this
possibility but retrying the whole computation.

The last option is the most sensible and better suited for our use case, and so
it is the one that we implement, this gave users the possibility to:
\begin{enumerate}
\item Complete their computation
\item Update the local images in order to always run stable containers
\end{enumerate}

In order to know which layer to delete from the file-system we store a reference
that map each layer to the images that use the layer itself.

These references are stored as metadata in a simple \texttt{.json} file.  We
store one of these reference file for each layer in the file-system.

Anytime a new image is added to the file-system we update the several
references files, adding for each layer in the image, a reference to the image
itself.

\begin{figure}
\begin{lstlisting}[caption={Algorithm to add an image reference to the layer metadata}, label={lst:add-image-reference-to-layer}]
Function AddReferenceToImage
        Pass In: LayerReference, ImageReference
        ReferenceFile := FindReferenceFile(LayerReference)
        if ReferenceFile exist
                References := LoadReferenceFromFile RefereceFile
                Add ImageReference to References
                Overwrite References to ReferenceFile
        else 
                References = ImageReferences
                Write References to ReferenceFile
        endif
EndFunction
\end{lstlisting}
\end{figure}

When we decide to remove an image, for any layer we check that it is used only
by the image we want to remove, if this is the case, we remove the layer, if it
is not the case we just remove the reference of the image.

\begin{figure}
\begin{lstlisting}[caption={Algorithm to remove an image from the file-system}, label={lst:remove-layer}]
Function RemoveLayer
        Pass In: LayerReference, ImageReference
        ReferenceFile := FindReferenceFile(LayerReference)
        References := LoadReferenceFromFile RefereceFile
        Remove ImageReference from References
        if size References == 0
                Remove Layer
        else
                Overwrite References to ReferenceFile
        endif
EndFunction
\end{lstlisting}
\end{figure}

In order to store both the metadata information about the layers (in particular
the "reference" file mentioned above) and the actual file-system of the layer
an additional directory structure is used. Below the directory called as the
digest of the layer there are two more directories: 
\begin{enumerate} 
        \item \texttt{layerfs/} directory that actually store the content of the layer
        \item \texttt{.metadata/} directory that stores the references to the image in a simple JSON encoded file, “origin.json”
\end{enumerate}

Of course, the recipe of the thin images is not concerned at all with the
content of the \texttt{.metadata/} directory.  Hence the recipe files points
directly to the \texttt{layerfs/} directory.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch/.layers.
.2 21.
.3 2100d...d7b7dbf.
.4 .metadata.
.5 origin.json \DTcomment{the reference file}.
.4 layerfs/ \ldots{} \begin{minipage}[t]{5cm}
        This directory contains the file-system of the layer itself and is the one that appears in the recipe of the thin-image
        \end{minipage}.
.3 217f7...601e9e7.
.4 .metadata.
.5 origin.json.
.4 layerfs/.
.2 c3.
.3 c300b...4190f83.
.4 .metadata.
.5 origin.json.
.4 layerfs/.
.3 c3683...53a1c45.
.4 .metadata.
.5 origin.json.
.4 layerfs/.
}
\caption{Complete visualization of the \texttt{.flat} directory}
\label{fig:docker-layer-structure}
\end{figure}

The complete structure for storing docker images is the one showed in
\ref{fig:docker-layer-structure}

\section{Keeping track of the work already done}
\label{sec:methodology-keep-track}

To avoid to perform duplicated work is necessary to keep track of which image
is already been added to the file-system. The same information may be used by
the users to know exactly what images is hosted in the file-system.

In order to know which image is already been converted we need to uniquely
identify each image, as already mention, using the combination of image name
and tag is not enough, since the tag are mutable.  Hence we rely on the digest
of the image.

The information about each image is stored into another top-level hidden
directory, \texttt{.metadata/}.

Inside the \texttt{.metadata/} folder we have others directories, one for each
hosted image.  Inside those directories there is a single file,
\texttt{manifest.json} that store the manifest of the image itself.

As already mentioned in \ref{subsec:singularity-docker-distribution} the
manifest contains the digest of the image itself.  Comparing the manifest
stored in the file-system with the manifest downloaded from the docker
registries is possible to understand if the image should be updated or nor.

The structure of the \texttt{.metadata/} folder is show in figure
\ref{fig:metadata-folder-structure}.

\begin{figure}
\dirtree{%
.1 /cvmfs/unpacked.cern.ch/.metadata.
.2 registry.hub.docker.com.
.3 library.
.4 python:latest.
.5 manifest.json \DTcomment{the manifest file}.
.4 r-base:latest.
.5 manifest.json.
.4 julia:latest.
.5 manifest.json.
.3 atlas.
.4 athena:latest.
.5 manifest.json.
}
\caption{Structure of the \texttt{.metadata/} directory}
\label{fig:metadata-folder-structure}
\end{figure}

\section{Closing remarks}

In this chapter we have introduced a file-system structure suitable to host
docker images that can be run using both Singularity and the docker thin-images
plugin.

We started by storing the unpacked images used by Singularity in a hierarchical
structure that recall the one of the Docker registries to enhance the
discoverability of the images itself. This approach however would have make
difficult to maintain the repository since we would not know the version of
each image unpacked. We overcome this issue storing the real unpacked images in
a hidden directory that embed the digest of the image itself and using symbolic
links to preserve the hierarchical structure. Too many unpacked images stored
under the same directories however would have put too much pressure on the
catalog system of CVMFS, hence we adopted the use of super-directories.

On the second part we analyzed how to store the layers used by the docker
thin-image plugin. A single layer can be used by multiple images this allowed
us to avoid repeating work but at the same time it makes more complex removing
an image from the file-system. We decided to keep a reference count to know
when is safe to actually delete a layer. The same issue of too many files under
the same sub-catalog arose also for storing the layers, we used the same
approach used for the unpacked images based on the use of super-directories.

The last section explored how we keep track of exactly which image is already
store in the file-system storing the image catalog in a hidden subdirectory.
