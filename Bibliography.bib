% Encoding: windows-1252

@online{docker:what,
	url = {https://web.archive.org/web/20181122172321/https://www.docker.com/resources/what-container},
	title = {What is a Container},
	author = {Docker Inc},
	urldate = {2018-11-22},
}

@online{docker:registry,
	url = {https://web.archive.org/web/20181122172148/https://docs.docker.com/registry/},
	title = {Docker Registry},
	author = {Docker Inc},
	urldate = {2018-11-22},
}

@online{docker:tag,
	url = {https://web.archive.org/web/20181122172506/https://docs.docker.com/engine/reference/commandline/tag/},
	title = {Docker tag},
	author = {Docker Inc},
	urldate = {2018-11-22},
}

@online{docker:overview,
	url = {https://web.archive.org/web/20181122172634/https://docs.docker.com/engine/docker-overview/},
	title = {Docker overview},
	author = {Docker Inc},
	urldate = {2018-11-22},
}

@online{docker:storage,
	url = {https://web.archive.org/web/20181122172916/https://docs.docker.com/storage/storagedriver/},
	title = {About storage drivers},
	author = {Docker Inc},
	urldate = {2018-11-22},
}

@online{docker:plugin,
	url = {https://web.archive.org/web/20181122174052/https://docs.docker.com/engine/extend/
},
	title = {Docker Engine managed plugin system},
	author = {Docker Inc},
	urldate = {2018-11-22},
}


@online{singularity:home,
	url = {https://web.archive.org/web/20181122154728/https://www.sylabs.io/},
	title = {Singularity},
	author = {Sylabs.io},
	urldate = {2018-11-22},
}

@online{singularity:docker,
	url = {https://web.archive.org/web/20181122174545/https://www.sylabs.io/guides/2.6/user-guide/singularity_and_docker.html},
	title = {Singularity and Docker},
	author = {Sylabs.io},
	urldate = {2018-11-22},
}

@online{singularity:run,
	url = {https://web.archive.org/web/20181122175328/https://www.sylabs.io/guides/2.6/user-guide/appendix.html#run},
	title = {Singularity Appendix | run},
	author = {Sylabs.io},
	urldate = {2018-11-22},
}

@online{grid:website,
	url = {https://web.archive.org/web/20181122162700/http://wlcg.web.cern.ch/},
	title = {{W}{L}{C}{G}},
	author = {CERN},
	urldate = {2018-11-22},
}

@online{oci,
	url = {https://web.archive.org/web/20181122170509/https://www.opencontainers.org/},
	title = {Open Containers Initiative},
	author = {The Linux Foundation},
	urldate = {2018-11-22},
}

@online{oci:image-spec,
	url = {https://web.archive.org/web/20181122170529/https://github.com/opencontainers/image-spec/blob/master/spec.md},
	title = {Image Format Specification},
	author = {The Linux Foundation},
	urldate = {2018-11-22},
}

@online{oci:image-manifest,
	url = {https://web.archive.org/web/20181122170617/https://github.com/opencontainers/image-spec/blob/master/manifest.md},
	title = {Image Manifest Specification},
	author = {The Linux Foundation},
	urldate = {2018-11-22},
}


@online{oci:image-filesystem,
	url = {https://web.archive.org/web/20181122170713/https://github.com/opencontainers/image-spec/blob/master/layer.md},
	title = {Image Layer Filesystem Changeset},
	author = {The Linux Foundation},
	urldate = {2018-11-22},
}

@online{oci:content,
	url = {https://web.archive.org/web/20181122171756/https://github.com/opencontainers/image-spec/blob/master/descriptor.md},
	title = {OCI Content Descriptors},
	author = {The Linux Foundation},
	urldate = {2018-11-22},
}

@online{fuse,
	url = {https://web.archive.org/web/20181124093000/https://github.com/libfuse/libfuse},
	title = {libfuse, The reference implementation of the Linux FUSE (Filesystem in Userspace) interface},
	author = {Nikolaus Rath},
	urldate = {2018-11-24},
}

@online{gocyclo,
	url = {https://web.archive.org/web/20181124194738/https://github.com/fzipp/gocyclo},
	title = {gocycle, Calculate cyclomatic complexities of functions in Go source code.},
	author = {fzipp},
	urldate = {2018-11-24},
}

@online{repository-manager,
	url = {https://web.archive.org/web/20181125115146/https://github.com/cvmfs/docker-graphdriver/tree/devel/repository-manager},
	title = {repository-manager, utlity to manage the unpacked.cern.ch CVMFS repository},
	author = {Simone Mosciatti},
	urldate = {2018-11-25},
}

@online{ansible,
	url = {https://web.archive.org/web/20181127163843/https://www.ansible.com/},
	title = {Ansible is Simple It Automation},
	author = {Red Hat. Inc},
	urldate = {2018-11-27},
}

@online{puppet,
	url = {https://web.archive.org/web/20181127164038/https://puppet.com/},
	title = {Deliver better software, faster | Puppet},
	author = {Puppet},
	urldate = {2018-11-27},
}

@online{containerd,
	url = {https://web.archive.org/web/20181128165122/https://containerd.io/},
	title = {containerd -An industry-standard container runtime with an emphasis on simplicity, robustness, and portability},
	author = {The containerd authors},
	urldate = {2018-11-28},
}

@online{beegfs,
	url = {https://web.archive.org/web/20181128172734/https://www.beegfs.io/content/},
	title = {BeeGFS - The Leading Parallel Cluster File System},
	author = {ThinkParQ and Fraunhofer},
	urldate = {2018-11-28},
}

@online{aws-s3,
	url = {https://web.archive.org/web/20181128173120/https://aws.amazon.com/s3/},
	title = {Cloud Object Storage | Store and Retrieve Data Anywhere | Amazon Simple Storage Service},
	author = {Amazon Web Services Inc},
	urldate = {2018-11-28},
}

@online{yum,
	url = {https://web.archive.org/web/20181128175354/http://yum.baseurl.org/},
	title = {yum - Yellowdog Updater, Modified},
	author = {Seth Vidal},
	urldate = {2018-11-28},
}

@online{apt,
	url = {https://web.archive.org/web/20181128175621/https://wiki.debian.org/Apt},
	title = {APT - Advanced Package Tool},
	author = {The Debian Project},
	urldate = {2018-11-28},
}

@inproceedings{slacker,
	author = {Tyler Harter and Brandon Salmon and Rose Liu and Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau},
	title = {Slacker: Fast Distribution with Lazy Docker Containers},
	booktitle = {14th {USENIX} Conference on File and Storage Technologies ({FAST} 16)},
	year = {2016},
	isbn = {978-1-931971-28-7},
	address = {Santa Clara, CA},
	pages = {181--195},
	url = {https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter},
	publisher = {{USENIX} Association},
}

@techreport{grid:update,
      author        = "Bird, I and Buncic, P and Carminati, F and Cattaneo, M and
                       Clarke, P and Fisk, I and Girone, M and Harvey, J and
                       Kersevan, B and Mato, P and Mount, R and Panzer-Steindel,
                       B",
      title         = "{Update of the Computing Models of the WLCG and the LHC
                       Experiments}",
      number        = "CERN-LHCC-2014-014. LCG-TDR-002",
      month         = "Apr",
      year          = "2014",
      reportNumber  = "CERN-LHCC-2014-014",
      url           = "http://cds.cern.ch/record/1695401",
}

@book{grid:report,
      author        = "Eck, Christoph and Knobloch, J and Robertson, Leslie and
                       Bird, I and Bos, K and Brook, N and DÃ¼llmann, D and Fisk,
                       I and Foster, D and Gibbard, B and Grandi, C and Grey, F
                       and Harvey, J and Heiss, A and Hemmer, F and Jarp, S and
                       Jones, R and Kelsey, D and Lamanna, M and Marten, H and
                       Mato-Vila, P and Ould-Saada, F and Panzer-Steindel, B and
                       Perini, L and Schutz, Y and Schwickerath, U and Shiers, J
                       and Wenaus, T",
      title         = "{LHC computing Grid: Technical Design Report. Version 1.06
                       (20 Jun 2005)}",
      publisher     = "CERN",
      address       = "Geneva",
      series        = "Technical Design Report LCG",
      year          = "2005",
      reportNumber  = "CERN-LHCC-2005-024",
      url           = "http://cds.cern.ch/record/840543",
}

@article{graphdriver-plugin,
  author={N Hardi and J Blomer and G Ganis and R Popescu},
  title={Making containers lazy with Docker and CernVM-FS},
  journal={Journal of Physics: Conference Series},
  volume={1085},
  number={3},
  pages={032019},
  url={http://stacks.iop.org/1742-6596/1085/i=3/a=032019},
  year={2018},
  abstract={Containerization technology is becoming more and more popular because it provides an efficient way to improve deployment flexibility by packaging up code into software micro-environments. Yet, containerization has limitations and one of the main ones is the fact that entire container images need to be transferred before they can be used. Container images can be seen as software stacks and High Energy Physics has long solved the distribution problem for large software stacks with CernVM-FS. CernVM-FS provides a global, shared software area, where clients only load the small subset of binaries that are accessed for any given compute job. In this paper, we propose a solution to the problem of efficient image distribution using CernVM-FS for storage and transport of container images. We chose to implement our solution for the Docker platform, due to its popularity and widespread use. To minimize the impact on existing workflows our implementation comes as a Docker plugin, meaning that users will continue to pull, run, modify, and store Docker images using standard Docker tools. We introduce the concept of a thin image, whose contents are served on demand from CernVM-FS repositories. Such behavior closely reassembles the lazy evaluation strategy in programming language theory. Our measurements confirm that the time before a task starts executing depends only on the size of the files actually used, minimizing the cold start-up time in all cases.}
}

@ARTICLE{cvmfs,
author={J. Blomer and P. Buncic and R. Meusel and G. Ganis and I. Sfiligoi and D. Thain},
journal={Computing in Science Engineering},
title={The Evolution of Global Scale Filesystems for Scientific Software Distribution},
year={2015},
volume={17},
number={6},
pages={61-71},
abstract={Delivering complex software across a worldwide distributed system is a major challenge in high-throughput scientific computing. The problem arises at different scales for many scientific communities that use grids, clouds, and distributed clusters to satisfy their computing needs. For high-energy physics (HEP) collaborations dealing with large amounts of data that rely on hundreds of thousands of cores spread around the world for data processing, the challenge is particularly acute. To serve the needs of the HEP community, several iterations were made to create a scalable, user-level filesystem that delivers software worldwide on a daily basis. The implementation was designed in 2006 to serve the needs of one experiment running on thousands of machines. Since that time, this idea evolved into a new production global-scale filesystem serving the needs of multiple science communities on hundreds of thousands of machines around the world.},
keywords={physics computing;software engineering;high-energy physics;scientific computing;worldwide distributed system;scientific software distribution;Software development;Metadata;Servers;Distributed processing;Scientific computing;distributed systems;distributed filesystems;scientific computing},
doi={10.1109/MCSE.2015.111},
ISSN={1521-9615},
month={Nov},}

@INPROCEEDINGS{FID,
author={W. Kangjin and Y. Yong and L. Ying and L. Hanmei and M. Lin},
booktitle={2017 IEEE 2nd International Workshops on Foundations and Applications of Self* Systems (FAS*W)},
title={FID: A Faster Image Distribution System for Docker Platform},
year={2017},
volume={},
number={},
pages={191-198},
abstract={Docker has been widely adopted in enterprise-level container environment. As an important part of Docker-based container ecosystem, Docker Registry provides the service of storing, distributing and managing Docker images, which is crucial to run Docker containers. In large-scale container platforms, deploying applications is prone to overburdening Docker Registry with flooding network traffic, and this situation may even cause failures of image services. In this paper, we present a new P2P-based large-scale image distribution system called FID (Faster Image Distribution), which is able to accelerate the speed of distributing Docker images by taking full advantage of the bandwidth of not only Docker Registry but also other nodes in the cluster. We implemented and validated FID on the enterprise-level cluster. The experiment results show that, compared to the native image distribution method, FID reduces at least 97% of network traffic for Docker Registry. Furthermore, it reduces 83.50% of distribution time on average when distributing images among 200 nodes, and particularly, reduces up to 91.35% of distribution time for the 500M image of Hadoop.},
keywords={cloud computing;operating systems (computers);peer-to-peer computing;public domain software;virtual storage;large-scale container platforms;Docker Registry;image services;FID;native image distribution method;faster image distribution system;Docker platform;enterprise-level container environment;Docker-based container ecosystem;P2P-based large-scale image distribution system;Docker images distribution;Containers;Engines;Bandwidth;Peer-to-peer computing;Servers;Business;Conferences;Docker;Docker Registry;Peer-to-Peer Network;Distributed Container System},
doi={10.1109/FAS-W.2017.147},
ISSN={},
month={Sept},}

@inproceedings {210500,
author = {Ali Anwar and Mohamed Mohamed and Vasily Tarasov and Michael Littley and Lukas Rupprecht and Yue Cheng and Nannan Zhao and Dimitrios Skourtis and Amit S. Warke and Heiko Ludwig and Dean Hildebrand and Ali R. Butt},
title = {Improving Docker Registry Design Based on Production Workload Analysis},
booktitle = {16th {USENIX} Conference on File and Storage Technologies ({FAST} 18)},
year = {2018},
isbn = {978-1-931971-42-3},
address = {Oakland, CA},
pages = {265--278},
url = {https://www.usenix.org/conference/fast18/presentation/anwar},
publisher = {{USENIX} Association},
}


  	
@article{grow-fs,
  author={G Compostella and S Pagan Griso and D Lucchesi and I Sfiligoi and D Thain},
  title={CDF software distribution on the Grid using Parrot},
  journal={Journal of Physics: Conference Series},
  volume={219},
  number={6},
  pages={062009},
  url={http://stacks.iop.org/1742-6596/219/i=6/a=062009},
  year={2010},
  abstract={Large international collaborations that use decentralized computing models are becoming a custom rather than an exception in High Energy Physics. A good computing model for such big collaborations has to deal with the distribution of the experiment-specific software around the world. When the CDF experiment developed its software infrastructure, most computing was done on dedicated clusters. As a result, libraries, configuration files and large executables were deployed over a shared file system. In order to adapt its computing model to the Grid, CDF decided to distribute its software to all European Grid sites using Parrot, a user-level application capable of attaching existing programs to remote I/O systems through the filesystem interface. This choice allows CDF to use just one centralized source of code and a scalable set of caches all around Europe to efficiently distribute its code and requires almost no interaction with the existing Grid middleware or with local system administrators. This system has been in production at CDF in Europe since almost two years. Here, we present CDF implementation of Parrot and some comments on its performances.}
}

@inproceedings{back-forth,
  title = {Going Back and Forth: Efficient Multi-Deployment and Multi-Snapshotting on Clouds},
  AUTHOR = {Nicolae, Bogdan and Bresnahan, John and Keahey, Kate and Antoniu, Gabriel},
  URL = {https://hal.inria.fr/inria-00570682},
  BOOKTITLE = {{The 20th International ACM Symposium on High-Performance Parallel and Distributed Computing (HPDC 2011)}},
  ADDRESS = {San Jos{\'e}, CA, United States},
  YEAR = {2011},
  MONTH = Jun,
  KEYWORDS = {Nimbus ; Grid'5000 ; cloud computing ; BlobSeer ; VM storage ; IaaS ; multi-snaphotting ; multi-deployment},
  PDF = {https://hal.inria.fr/inria-00570682/file/final-paper.pdf},
  HAL_ID = {inria-00570682},
  HAL_VERSION = {v1},
}

@INPROCEEDINGS{provisioning,
author={J. O. Benson and J. J. Prevost and P. Rad},
booktitle={2016 Annual IEEE Systems Conference (SysCon)},
title={Survey of automated software deployment for computational and engineering research},
year={2016},
volume={},
number={},
pages={1-6},
keywords={application program interfaces;cloud computing;research and development;service-oriented architecture;automated software deployment;engineering research;cloud hosting providers;cloud technology;on demand cloud services;public providers;cloud infrastructure services;infrastructure resources;load surges;multicloud application deployment controller;cloud utility service;cloud providers API;Chef;SaltStack;Ansible;application automation;cloud computing services;Cloud computing;Servers;Companies;Imaging;Operating systems;Software packages;Distributed computing;Software;Open source software;Public domain software;Software as a service;Software maintenance;Software packages},
doi={10.1109/SYSCON.2016.7490666},
ISSN={},
month={April},}

@article{andrew,
  title={Andrew: A Distributed Personal Computing Environment},
  author={James H. Morris and Mahadev Satyanarayanan and Michael H. Conner and John H. Howard and David S. H. Rosenthal and F. Donelson Smith},
  journal={Commun. ACM},
  year={1986},
  volume={29},
  pages={184-201}
}

@thesis{jakob:cvmfs,
      author        = "Blomer, Jakob and Buncic, Predrag and Fuhrmann, Thomas",
      title         = "{Decentralized Data Storage and Processing in the Context
                       of the LHC Experiments at CERN}",
      month         = "Dec",
      year          = "2011",
      reportNumber  = "CERN-THESIS-2011-251",
      url           = "https://cds.cern.ch/record/1462821",
      note          = "Presented 05 Jul 2012",
}

